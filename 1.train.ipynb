{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326d3fa6",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94a0c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 11 16:55:12 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX           On   | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 60%   83C    P2   280W / 280W |  23384MiB / 24220MiB |     37%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN RTX           On   | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 40%   48C    P2    54W / 280W |  23906MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN RTX           On   | 00000000:1D:00.0 Off |                  N/A |\n",
      "| 41%   41C    P8    10W / 280W |    166MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN RTX           On   | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 41%   40C    P8     6W / 280W |      3MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  TITAN RTX           On   | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 41%   29C    P8    16W / 280W |   4985MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  TITAN RTX           On   | 00000000:3F:00.0 Off |                  N/A |\n",
      "| 41%   31C    P8     4W / 280W |      3MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  TITAN RTX           On   | 00000000:40:00.0 Off |                  N/A |\n",
      "| 40%   33C    P8     4W / 280W |  17021MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  TITAN RTX           On   | 00000000:41:00.0 Off |                  N/A |\n",
      "| 41%   34C    P8    14W / 280W |      3MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpu status\n",
    "!nvidia-smi\n",
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e008be-6ac3-446d-895d-f429c226fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "!pip install -r requirements.txt --user --upgrade #--quiet -U\n",
    "# !apt updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c19a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo rm -r logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c7f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls logs/Retina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefdfaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log into weight and bias, if error occurs, please reset your jupyter kernal\n",
    "# !sudo pip install wandb --upgrade --user\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e9bd2",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db84e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(batch_size=24, data_cropsize=None, data_dir='../Retina/dataset/DRIVE', data_module='dataset', data_padsize=None, data_patchsize='128_128', data_resize=None, experiment_name=None, lossfn='BoundaryCELoss', lr=0.001, net='waveletunet_att', net_ckpt=None, net_inputch=3, net_nnblock=True, net_norm='batch', net_outputch=3, net_supervision=True, precision=16, project='Retina') \n",
      "\n",
      "project Retina\n",
      "Current Experiment: DatasetDRIVE_Netwaveletunet_att_Netnormbatch_Netinputch3_Netoutputch3_LossBoundaryCELoss_Lr0.001_Precision16_Patchsize128_128_PrefixNone_ \n",
      " ****************************************************************************************************\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeewonshin\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mDatasetDRIVE_Netwaveletunet_att_Netnormbatch_Netinputch3_Netoutputch3_LossBoundaryCELoss_Lr0.001_Precision16_Patchsize128_128_PrefixNone_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/keewonshin/Retina\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/keewonshin/Retina/runs/3m0v29g6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in logs/wandb/run-20210811_165526-3m0v29g6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "total counts of dataset x 18, y 18\n",
      "total counts of dataset x 2, y 2\n",
      "total counts of dataset x 20, y 20\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [7]\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n",
      "\n",
      "  | Name   | Type            | Params\n",
      "-------------------------------------------\n",
      "0 | lossfn | BoundaryCELoss  | 0     \n",
      "1 | net    | waveletunet_att | 29.9 M\n",
      "2 | metric | F1              | 0     \n",
      "-------------------------------------------\n",
      "29.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "29.9 M    Total params\n",
      "119.721   Total estimated model params size (MB)\n",
      "Epoch 0:  50%|▌| 1/2 [00:06<00:06,  6.90s/it, loss=2.04, v_num=29g6, loss_val=2./home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py:100: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5\n",
      "  warning_cache.deprecation(\n",
      "\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 2/2 [00:14<00:00,  7.32s/it, loss=2.040, v_num=29g6, loss_val=2\u001b[A\n",
      "                                                                                \u001b[AEpoch 0, global step 0: metric_val reached 0.08183 (best 0.08183), saving model to \"logs/Retina/3m0v29g6/checkpoints/epoch=0000-loss_val=2.1152-metric_val=0.0818.ckpt\" as top 3\n"
     ]
    }
   ],
   "source": [
    "# run training\n",
    "!CUDA_VISIBLE_DEVICES=7 python train.py --project Retina \\\n",
    "                                        --data_dir ../Retina/dataset/DRIVE \\\n",
    "                                        --data_module dataset \\\n",
    "                                        --net waveletunet_att \\\n",
    "                                        --net_inputch 3 \\\n",
    "                                        --net_outputch 3 \\\n",
    "                                        --net_norm batch \\\n",
    "                                        --net_nnblock True \\\n",
    "                                        --net_supervision True \\\n",
    "                                        --lossfn BoundaryCELoss \\\n",
    "                                        --precision 16 \\\n",
    "                                        --data_patchsize 128_128 \\\n",
    "                                        --batch_size 24 \\\n",
    "                                        --lr 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960e146",
   "metadata": {},
   "source": [
    "# Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b48c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sweep using sweep.yaml\n",
    "!python -m wandb sweep sweep.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sweeps\n",
    "# !CUDA_VISIBLE_DEVICES=0 python -m wandb agent [your_sweep_address]\n",
    "!CUDA_VISIBLE_DEVICES=6 python -m wandb agent keewonshin/Retina/yy8sgak3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de4561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
