{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7878d7",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/kevinkwshin/Mytorch\n",
    "# %cd Mytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r logs/Mytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ceff5121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 29 00:18:57 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:18:00.0 Off |                  Off |\n",
      "| 33%   49C    P2   229W / 260W |  23215MiB / 48601MiB |     91%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     On   | 00000000:3B:00.0 Off |                  Off |\n",
      "| 33%   23C    P8    15W / 260W |   1240MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 8000     On   | 00000000:86:00.0 Off |                  Off |\n",
      "| 33%   22C    P8     7W / 260W |  38248MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     On   | 00000000:AF:00.0 Off |                  Off |\n",
      "| 33%   22C    P8    18W / 260W |  35382MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# gpu status\n",
    "!nvidia-smi\n",
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3edc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in '/home/kevin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/home/kevin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/home/kevin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.9.1 requires torch==1.8.1, but you have torch 1.8.0 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "!pip install -r requirements.txt --user --upgrade --quiet -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6300bafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/kevin/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log into weight and bias, if error occurs, please reset your jupyter kernal\n",
    "# !pip install wandb --upgrade --user\n",
    "# !sudo pip install wandb kornia --upgrade\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc7ace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==1.8.1\n",
      "  Downloading torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl (804.1 MB)\n",
      "     |████████████████████████████████| 804.1 MB 12 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/kevin/.local/lib/python3.8/site-packages (from torch==1.8.1) (1.21.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1) (3.10.0.2)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.0\n",
      "    Uninstalling torch-1.8.0:\n",
      "      Successfully uninstalled torch-1.8.0\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/home/kevin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.9.0 requires torch==1.8.0, but you have torch 1.8.1 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.8.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytorch_lightning==1.4.0\n",
    "!pip install torch==1.8.1\n",
    "# torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39fa84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision, monai\n",
    "torch.__version__, torchvision.__version__, monai.__version__\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "# pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e177ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de45179",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35572ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707499cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install ujson \n",
    "# !sudo apt-get update\n",
    "# !sudo apt install -y libsm6 libxext6 libxrender-dev libgl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c198b67a-3a3d-40fb-9114-b1801f790a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ba34c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(activation_T=4, batch_size=24, data_cropsize=None, data_dir='DRIVE2CH', data_module='dataset_CLAHE', data_padsize=None, data_patchsize='128_128', data_resize='584_565', experiment_name='Base', lossfn='DiceCELoss', lr=0.001, net_bayesian=0.2, net_ckpt=None, net_encoder_name='resnet50', net_inputch=3, net_name='monai_unetr', net_nnblock=True, net_norm='batch', net_outputch=2, net_rcnn=False, net_skipatt=True, net_supervision=True, net_wavelet=True, precision=32, project='Retina') \n",
      "\n",
      "project Retina\n",
      "Current Experiment: DatasetDRIVE2CH_Netmonai_unetr_NetEncoderresnet50_LossDiceCELoss_Precision32_Patchsize128_128_PrefixBase_ \n",
      " ****************************************************************************************************\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeewonshin\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mDatasetDRIVE2CH_Netmonai_unetr_NetEncoderresnet50_LossDiceCELoss_Precision32_Patchsize128_128_PrefixBase_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/keewonshin/Retina\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/keewonshin/Retina/runs/tdrcp56r\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in logs/wandb/run-20211129_001908-tdrcp56r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "total counts of dataset x 36, y 17\n",
      "total counts of dataset x 6, y 3\n",
      "total counts of dataset x 40, y 20\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type        | Params\n",
      "---------------------------------------\n",
      "0 | lossfn | DiceCELoss  | 0     \n",
      "1 | net    | monai_unetr | 87.6 M\n",
      "---------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.274   Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"train.py\", line 518, in <module>\n",
      "    main(args)\n",
      "  File \"train.py\", line 505, in main\n",
      "    trainer.fit(model,datamodule=myData)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 737, in fit\n",
      "    self._call_and_handle_interrupt(\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 682, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 772, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1195, in _run\n",
      "    self._dispatch()\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1274, in _dispatch\n",
      "    self.training_type_plugin.start_training(self)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\n",
      "    self._results = trainer.run_stage()\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1284, in run_stage\n",
      "    return self._run_train()\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1306, in _run_train\n",
      "    self._run_sanity_check(self.lightning_module)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1370, in _run_sanity_check\n",
      "    self._evaluation_loop.run()\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 109, in advance\n",
      "    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 140, in run\n",
      "    self.on_run_start(*args, **kwargs)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 86, in on_run_start\n",
      "    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py\", line 121, in _update_dataloader_iter\n",
      "    dataloader_iter = enumerate(data_fetcher, batch_idx)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py\", line 199, in __iter__\n",
      "    self.prefetching(self.prefetch_batches)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py\", line 258, in prefetching\n",
      "    self._fetch_next_batch()\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py\", line 300, in _fetch_next_batch\n",
      "    batch = next(self.dataloader_iter)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 517, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1199, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1225, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/torch/_utils.py\", line 429, in reraise\n",
      "    raise self.exc_type(msg)\n",
      "IndexError: Caught IndexError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/kevin/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/workspace/Mytorch/datasets.py\", line 89, in __getitem__\n",
      "    y = cv2.imread(self.y_list[idx])\n",
      "IndexError: list index out of range\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 12417... (failed 1). Press ctrl-c to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mDatasetDRIVE2CH_Netmonai_unetr_NetEncoderresnet50_LossDiceCELoss_Precision32_Patchsize128_128_PrefixBase_\u001b[0m: \u001b[34mhttps://wandb.ai/keewonshin/Retina/runs/tdrcp56r\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: logs/wandb/run-20211129_001908-tdrcp56r/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    }
   ],
   "source": [
    " # run training\n",
    "!CUDA_VISIBLE_DEVICES=0 python train.py --project Retina \\\n",
    "                                        --data_dir DRIVE2CH \\\n",
    "                                        --data_module dataset_CLAHE \\\n",
    "                                        --data_padsize None \\\n",
    "                                        --data_cropsize None \\\n",
    "                                        --data_resize 584_565 \\\n",
    "                                        --data_patchsize 128_128 \\\n",
    "                                        --activation_T 4 \\\n",
    "                                        --net_name monai_unetr \\\n",
    "                                        --net_inputch 3 \\\n",
    "                                        --net_outputch 2 \\\n",
    "                                        --net_encoder_name resnet50 \\\n",
    "                                        --net_norm batch \\\n",
    "                                        --net_bayesian 0.2 \\\n",
    "                                        --net_rcnn False \\\n",
    "                                        --net_skipatt True \\\n",
    "                                        --net_nnblock True\\\n",
    "                                        --net_wavelet True \\\n",
    "                                        --net_supervision True \\\n",
    "                                        --lossfn DiceCELoss \\\n",
    "                                        --batch_size 24 \\\n",
    "                                        --lr 1e-3 \\\n",
    "                                        --precision 32 \\\n",
    "                                        --experiment_name Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e35b3e",
   "metadata": {},
   "source": [
    "# Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sweep using sweep.yaml\n",
    "!python -m wandb sweep sweep.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db734b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sweeps\n",
    "# !CUDA_VISIBLE_DEVICES=0 python -m wandb agent [your_sweep_address]\n",
    "!CUDA_VISIBLE_DEVICES=0 python -m wandb agent keewonshin/Retina/ch4zygfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, natsort, cv2\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "path = '../Retina/dataset/HRF/retina_hrf_yjh_modified/'\n",
    "list_folder = glob.glob(path+'*')\n",
    "# list_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# list_files = glob.glob(path+'*.png')\n",
    "# path = '../Retina/dataset/HRF/retina_hrf_yjh_modified/'\n",
    "# for idx in range(len(list_files)):\n",
    "#     src = list_files[idx]\n",
    "#     dst = path+list_files[idx].split('/')[-1][:-10]+'/'+list_files[idx].split('/')[-1]\n",
    "# #     shu\n",
    "#     print(src,dst)\n",
    "# # list_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_O[...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_G[...,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20997d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(1):\n",
    "for idx in range(len(list_folder)):\n",
    "    O = glob.glob(list_folder[idx]+'/*manual.png')\n",
    "    Y = glob.glob(list_folder[idx]+'/*_Y*')\n",
    "    G = glob.glob(list_folder[idx]+'/*_G*')\n",
    "    \n",
    "    img_O = cv2.imread(O[0])\n",
    "    img_Y = cv2.imread(Y[0])\n",
    "    img_G = cv2.imread(G[0])\n",
    "    img_O[img_O!=0] = 1\n",
    "    img_Y[img_Y!=0] = 1\n",
    "    img_G[img_G!=0] = 1\n",
    "\n",
    "    img_result = np.zeros_like(img_O)\n",
    "    img_result[...,0] = img_O[...,0] + img_Y[...,1]\n",
    "    img_result[...,1] = img_O[...,2] + img_Y[...,1]\n",
    "    img_result[...,2] = img_O[...,1] + img_G[...,1]\n",
    "\n",
    "    img_result[img_result!=0] = 1\n",
    "    \n",
    "    plt.figure(figsize=(18,18))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(img_O[...,0])\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(img_result[...,0])\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(img_O[...,2])\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(img_result[...,1])\n",
    "    plt.show()\n",
    "\n",
    "    print(np.unique(img_O), np.unique(img_Y))\n",
    "    fname = list_folder[idx].split('/')[-1]\n",
    "    np.save(fname, img_result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93543ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pylab as plt\n",
    "# plt.imshow(img_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d317ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cade85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "plt.imshow(img_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(img_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0c262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
